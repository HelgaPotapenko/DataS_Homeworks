{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#NLP2_2-identify-the-unique-domain-names\" data-toc-modified-id=\"NLP2_2-identify-the-unique-domain-names-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>NLP2_2 identify the unique domain names</a></span></li><li><span><a href=\"#NLP2_3-(дз1):-Реализовать-stemming,-lemmatization-&amp;-BoW\" data-toc-modified-id=\"NLP2_3-(дз1):-Реализовать-stemming,-lemmatization-&amp;-BoW-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>NLP2_3 (дз1): Реализовать stemming, lemmatization &amp; BoW</a></span></li><li><span><a href=\"#NLP2_4-(дополнительно)-Реализовать-классификатор-токсичных-комментариев-tfidf-на-базе-датасета-(если-не-успели-на-классном-занятии)\" data-toc-modified-id=\"NLP2_4-(дополнительно)-Реализовать-классификатор-токсичных-комментариев-tfidf-на-базе-датасета-(если-не-успели-на-классном-занятии)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>NLP2_4 (дополнительно) Реализовать классификатор токсичных комментариев tfidf на базе датасета (если не успели на классном занятии)</a></span></li><li><span><a href=\"#Реализовать-tfidf-датасета-./quora.txt,-посчитать-косинусное-расстояние\" data-toc-modified-id=\"Реализовать-tfidf-датасета-./quora.txt,-посчитать-косинусное-расстояние-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Реализовать tfidf датасета ./quora.txt, посчитать косинусное расстояние</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework2_NLP Школа DataScientist, декабрь 2022, Андреева Ольга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKzbIfdq0CKr"
   },
   "source": [
    "# NLP2_2 identify the unique domain names\n",
    "https://www.hackerrank.com/challenges/detect-the-domain-name/problem?isFullScreen=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3astrospeak.lkh.com;b.com;w3.org;zarabol.rediff.com\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "in_list = ['1027'\n",
    "            ,'\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">'\n",
    "            ,'\"http://3astrospeak.lkh.com?\"'\n",
    "           ,'url=___http://zarabol.rediff.com___&cmp=za'\n",
    "           ,'\"http://b.com\"'\n",
    "          ]\n",
    "#n=int(input())    \n",
    "#in_list = [input() for i in range(n)]\n",
    "\n",
    "c_tmplt1 = '[http|https]://(www\\.|ww2\\.)?(([a-z0-9][a-z0-9\\-]*\\.)+[a-z0-9\\-]+)'\n",
    "out_set=set()\n",
    "for v_str in  in_list:\n",
    "    v_finded = re.findall(c_tmplt1 , v_str, flags=re.IGNORECASE)\n",
    "    for v_item in v_finded:\n",
    "        v_dn = v_item[1].strip()\n",
    "        out_set.add(v_dn)\n",
    "\n",
    "out_set=sorted(out_set)\n",
    "print(\";\".join(out_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Op8qPHa8J68_"
   },
   "source": [
    "# NLP2_3 (дз1): Реализовать stemming, lemmatization & BoW\n",
    "на следующем датасете: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK (Russian Toxic-abuse comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"./csv4nlp/labeled.csv\", encoding=\"utf-8\")\n",
    "#df_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing: 0:00:01.160306\n",
      "steaming: 0:00:39.010588\n",
      "lemmatization: 0:03:29.519595\n",
      "#steaming after lemmatization: 0:00:39.025824\n"
     ]
    }
   ],
   "source": [
    "#tokenizing\n",
    "start_time=datetime.now()\n",
    "tokenizer = WordPunctTokenizer()\n",
    "df_data['tokens'] = df_data.apply(lambda x: list(w.lower() for w in tokenizer.tokenize(x.comment)), axis=1)\n",
    "print('tokenizing:', datetime.now()-start_time)\n",
    "\n",
    "#steaming\n",
    "start_time=datetime.now()\n",
    "snb = SnowballStemmer(language=\"russian\")    \n",
    "df_data['steams'] = df_data.apply(lambda x: list(snb.stem(w) for w in (x.tokens)), axis=1)\n",
    "print('steaming:', datetime.now()-start_time)\n",
    "\n",
    "#lemmatization\n",
    "start_time=datetime.now()\n",
    "lemmatizer = MorphAnalyzer()\n",
    "df_data['lemmas'] = df_data.apply(lambda x: list(lemmatizer.normal_forms(w)[0] for w in (x.tokens)), axis=1)\n",
    "print('lemmatization:', datetime.now()-start_time)\n",
    "\n",
    "#steaming after lemmatization\n",
    "start_time=datetime.now()\n",
    "snb = SnowballStemmer(language=\"russian\")\n",
    "df_data['lem_steam'] = df_data.apply(lambda x: list(snb.stem(w) for w in (x.lemmas)), axis=1)\n",
    "print('#steaming after lemmatization:', datetime.now()-start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#word_filter: 0:00:05.107695\n"
     ]
    }
   ],
   "source": [
    "#special_char=[\",\",\":\",\" \",\";\",\".\",\"?\",\"-\", '...','!']\n",
    "stop_words_rus = stopwords.words(\"russian\")\n",
    "stop_words_eng= stopwords.words('english')\n",
    "stop_words = stop_words_rus + stop_words_eng\n",
    "word_marks = r'.+[а-яa-z]' #for filtre not-word tokens\n",
    "\n",
    "def word_filter (p_words, p_filter):\n",
    "    res = []\n",
    "    for w in p_words:\n",
    "        if (w not in p_filter) and (re.match(word_marks, w)):\n",
    "            res.append(re.sub(r'\\d+', '', w))\n",
    "    return res\n",
    "\n",
    "start_time=datetime.now()\n",
    "df_data['clear'] = df_data.apply(lambda x: word_filter(x.lem_steam, stop_words), axis=1)\n",
    "print('#word_filter:', datetime.now()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "      <th>tokens</th>\n",
       "      <th>steams</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>lem_steam</th>\n",
       "      <th>clear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[верблюдов, -, то, за, что, ?, дебилы, ,, бл, ...</td>\n",
       "      <td>[верблюд, -, то, за, что, ?, дебил, ,, бл, ...]</td>\n",
       "      <td>[верблюд, -, то, за, что, ?, дебил, ,, бл, ...]</td>\n",
       "      <td>[верблюд, -, то, за, что, ?, деб, ,, бл, ...]</td>\n",
       "      <td>[верблюд, деб, бл]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[хохлы, ,, это, отдушина, затюканого, россияни...</td>\n",
       "      <td>[хохл, ,, эт, отдушин, затюкан, россиянин, ,, ...</td>\n",
       "      <td>[хохол, ,, это, отдушина, затюканый, россиянин...</td>\n",
       "      <td>[хохол, ,, эт, отдушин, затюкан, россиянин, ,,...</td>\n",
       "      <td>[хохол, эт, отдушин, затюкан, россиянин, мол, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Собаке - собачья смерть\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[собаке, -, собачья, смерть]</td>\n",
       "      <td>[собак, -, собач, смерт]</td>\n",
       "      <td>[собака, -, собачий, смерть]</td>\n",
       "      <td>[собак, -, собач, смерт]</td>\n",
       "      <td>[собак, собач, смерт]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[страницу, обнови, ,, дебил, ., это, тоже, не,...</td>\n",
       "      <td>[страниц, обнов, ,, деб, ., эт, тож, не, оскор...</td>\n",
       "      <td>[страница, обновить, ,, дебил, ., это, тоже, н...</td>\n",
       "      <td>[страниц, обнов, ,, деб, ., эт, тож, не, оскор...</td>\n",
       "      <td>[страниц, обнов, деб, эт, тож, оскорблен, дока...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[тебя, не, убедил, 6, -, страничный, пдф, в, т...</td>\n",
       "      <td>[теб, не, убед, 6, -, страничн, пдф, в, том, ,...</td>\n",
       "      <td>[ты, не, убедить, 6, -, страничный, пдф, в, то...</td>\n",
       "      <td>[ты, не, убед, 6, -, страничн, пдф, в, тот, ,,...</td>\n",
       "      <td>[убед, страничн, пдф, скрипал, отрав, росс, ан...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  toxic  \\\n",
       "0               Верблюдов-то за что? Дебилы, бл...\\n    1.0   \n",
       "1  Хохлы, это отдушина затюканого россиянина, мол...    1.0   \n",
       "2                          Собаке - собачья смерть\\n    1.0   \n",
       "3  Страницу обнови, дебил. Это тоже не оскорблени...    1.0   \n",
       "4  тебя не убедил 6-страничный пдф в том, что Скр...    1.0   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [верблюдов, -, то, за, что, ?, дебилы, ,, бл, ...   \n",
       "1  [хохлы, ,, это, отдушина, затюканого, россияни...   \n",
       "2                       [собаке, -, собачья, смерть]   \n",
       "3  [страницу, обнови, ,, дебил, ., это, тоже, не,...   \n",
       "4  [тебя, не, убедил, 6, -, страничный, пдф, в, т...   \n",
       "\n",
       "                                              steams  \\\n",
       "0    [верблюд, -, то, за, что, ?, дебил, ,, бл, ...]   \n",
       "1  [хохл, ,, эт, отдушин, затюкан, россиянин, ,, ...   \n",
       "2                           [собак, -, собач, смерт]   \n",
       "3  [страниц, обнов, ,, деб, ., эт, тож, не, оскор...   \n",
       "4  [теб, не, убед, 6, -, страничн, пдф, в, том, ,...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0    [верблюд, -, то, за, что, ?, дебил, ,, бл, ...]   \n",
       "1  [хохол, ,, это, отдушина, затюканый, россиянин...   \n",
       "2                       [собака, -, собачий, смерть]   \n",
       "3  [страница, обновить, ,, дебил, ., это, тоже, н...   \n",
       "4  [ты, не, убедить, 6, -, страничный, пдф, в, то...   \n",
       "\n",
       "                                           lem_steam  \\\n",
       "0      [верблюд, -, то, за, что, ?, деб, ,, бл, ...]   \n",
       "1  [хохол, ,, эт, отдушин, затюкан, россиянин, ,,...   \n",
       "2                           [собак, -, собач, смерт]   \n",
       "3  [страниц, обнов, ,, деб, ., эт, тож, не, оскор...   \n",
       "4  [ты, не, убед, 6, -, страничн, пдф, в, тот, ,,...   \n",
       "\n",
       "                                               clear  \n",
       "0                                 [верблюд, деб, бл]  \n",
       "1  [хохол, эт, отдушин, затюкан, россиянин, мол, ...  \n",
       "2                              [собак, собач, смерт]  \n",
       "3  [страниц, обнов, деб, эт, тож, оскорблен, дока...  \n",
       "4  [убед, страничн, пдф, скрипал, отрав, росс, ан...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258860\n",
      "29405\n",
      "Wall time: 304 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = []\n",
    "for x in df_data.clear:\n",
    "    vocab += x\n",
    "print (len(sorted(vocab)))\n",
    "\n",
    "vocab_unique = list(set(vocab))\n",
    "\n",
    "print (len(sorted(vocab_unique)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['каса', 'касаем', 'касательн', 'каск', 'каскад', 'касп', 'касперск', 'каспийск', 'касс', 'кассет']\n"
     ]
    }
   ],
   "source": [
    "print (sorted(vocab_unique)[10000:10010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(p_words, p_vocab):\n",
    "    ''' This function takes list of words in a sentence as input \n",
    "    and returns a vector of size of filtered_vocab.It puts 0 if the \n",
    "    word is not present in tokens and count of token if present.'''\n",
    "    vector=[]\n",
    "    for w in p_vocab:\n",
    "        vector.append(p_words.count(w))\n",
    "    return sparse.coo_matrix(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#get bag of words: 0:06:52.070272\n"
     ]
    }
   ],
   "source": [
    "#BoW\n",
    "start_time=datetime.now()\n",
    "df_data['BoW'] = df_data.apply(lambda x: vectorize(x.clear, vocab_unique), axis=1)\n",
    "print('#get bag of words:', datetime.now()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#get bag of words with CountVectorizer: 0:00:01.936202\n"
     ]
    }
   ],
   "source": [
    "#BoW with  CountVectorizer\n",
    "start_time=datetime.now()\n",
    "\n",
    "cnt_vctrzr = CountVectorizer(lowercase=False, vocabulary=vocab_unique)\n",
    "BoW_sklearn = cnt_vctrzr.fit_transform(df_data.clear.apply(lambda x: ' '.join(x)))\n",
    "df_data['BoW_sklearn'] = [BoW_sklearn.getrow(i) for i in df_data.index]\n",
    "print('#get bag of words with CountVectorizer:', datetime.now()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>clear</th>\n",
       "      <th>BoW</th>\n",
       "      <th>BoW_sklearn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
       "      <td>[верблюд, деб, бл]</td>\n",
       "      <td>(0, 5172)\\t1\\n  (0, 16868)\\t1\\n  (0, 18339)\\t1</td>\n",
       "      <td>(0, 5172)\\t1\\n  (0, 16868)\\t1\\n  (0, 18339)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было, кисель их бы придумал.\\n</td>\n",
       "      <td>[хохол, эт, отдушин, затюкан, россиянин, мол, вон, хохл, ещ, плох, есл, хохл, быт, кисел, придума]</td>\n",
       "      <td>(0, 1310)\\t1\\n  (0, 2689)\\t1\\n  (0, 7025)\\t1\\n  (0, 9678)\\t1\\n  (0, 10804)\\t1\\n  (0, 11040)\\t1\\n  (0, 13034)\\t1\\n  (0, 14305)\\t1\\n  (0, 16492)\\t1\\n  (0, 20536)\\t1\\n  (0, 20980)\\t2\\n  (0, 21643)\\t1\\n  (0, 23776)\\t1\\n  (0, 27074)\\t1</td>\n",
       "      <td>(0, 1310)\\t1\\n  (0, 2689)\\t1\\n  (0, 7025)\\t1\\n  (0, 9678)\\t1\\n  (0, 10804)\\t1\\n  (0, 11040)\\t1\\n  (0, 13034)\\t1\\n  (0, 14305)\\t1\\n  (0, 16492)\\t1\\n  (0, 20536)\\t1\\n  (0, 20980)\\t2\\n  (0, 21643)\\t1\\n  (0, 23776)\\t1\\n  (0, 27074)\\t1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                      comment  \\\n",
       "0                                                                                        Верблюдов-то за что? Дебилы, бл...\\n   \n",
       "1  Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было, кисель их бы придумал.\\n   \n",
       "\n",
       "                                                                                                clear  \\\n",
       "0                                                                                  [верблюд, деб, бл]   \n",
       "1  [хохол, эт, отдушин, затюкан, россиянин, мол, вон, хохл, ещ, плох, есл, хохл, быт, кисел, придума]   \n",
       "\n",
       "                                                                                                                                                                                                                                        BoW  \\\n",
       "0                                                                                                                                                                                            (0, 5172)\\t1\\n  (0, 16868)\\t1\\n  (0, 18339)\\t1   \n",
       "1    (0, 1310)\\t1\\n  (0, 2689)\\t1\\n  (0, 7025)\\t1\\n  (0, 9678)\\t1\\n  (0, 10804)\\t1\\n  (0, 11040)\\t1\\n  (0, 13034)\\t1\\n  (0, 14305)\\t1\\n  (0, 16492)\\t1\\n  (0, 20536)\\t1\\n  (0, 20980)\\t2\\n  (0, 21643)\\t1\\n  (0, 23776)\\t1\\n  (0, 27074)\\t1   \n",
       "\n",
       "                                                                                                                                                                                                                                BoW_sklearn  \n",
       "0                                                                                                                                                                                            (0, 5172)\\t1\\n  (0, 16868)\\t1\\n  (0, 18339)\\t1  \n",
       "1    (0, 1310)\\t1\\n  (0, 2689)\\t1\\n  (0, 7025)\\t1\\n  (0, 9678)\\t1\\n  (0, 10804)\\t1\\n  (0, 11040)\\t1\\n  (0, 13034)\\t1\\n  (0, 14305)\\t1\\n  (0, 16492)\\t1\\n  (0, 20536)\\t1\\n  (0, 20980)\\t2\\n  (0, 21643)\\t1\\n  (0, 23776)\\t1\\n  (0, 27074)\\t1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', None)\n",
    "df_data[['comment','clear','BoW','BoW_sklearn']].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5DQQnoU1bXY"
   },
   "source": [
    "# NLP2_4 (дополнительно) Реализовать классификатор токсичных комментариев tfidf на базе датасета (если не успели на классном занятии)\n",
    "https://www.kaggle.com/datasets/blackmoon/russian-language-toxic-comments  \n",
    "\n",
    "Дубликат файла: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "w=tfidf_transformer.fit(BoW_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>невролог</th>\n",
       "      <td>9.477274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>карандаш</th>\n",
       "      <td>8.784126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          idf_weights\n",
       "невролог     9.477274\n",
       "карандаш     8.784126"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cnt_vctrzr.get_feature_names_out(),columns=[\"idf_weights\"]) \n",
    "df_idf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#get tfidf: 0:00:04.599307\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vector = tfidf_transformer.transform(BoW_sklearn)\n",
    "\n",
    "start_time=datetime.now()\n",
    "tfidf_list = [sparse.coo_matrix(tf_idf_vector[i])  for i in df_data.index]\n",
    "#print(tfidf_list[0])\n",
    "df_data['TFIDF'] = tfidf_list\n",
    "print('#get tfidf:', datetime.now()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>clear</th>\n",
       "      <th>BoW_sklearn</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
       "      <td>[верблюд, деб, бл]</td>\n",
       "      <td>(0, 5172)\\t1\\n  (0, 16868)\\t1\\n  (0, 18339)\\t1</td>\n",
       "      <td>(0, 18339)\\t0.4228355715889868\\n  (0, 16868)\\t0.6101326664930184\\n  (0, 5172)\\t0.6700359756588701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было, кисель их бы придумал.\\n</td>\n",
       "      <td>[хохол, эт, отдушин, затюкан, россиянин, мол, вон, хохл, ещ, плох, есл, хохл, быт, кисел, придума]</td>\n",
       "      <td>(0, 1310)\\t1\\n  (0, 2689)\\t1\\n  (0, 7025)\\t1\\n  (0, 9678)\\t1\\n  (0, 10804)\\t1\\n  (0, 11040)\\t1\\n  (0, 13034)\\t1\\n  (0, 14305)\\t1\\n  (0, 16492)\\t1\\n  (0, 20536)\\t1\\n  (0, 20980)\\t2\\n  (0, 21643)\\t1\\n  (0, 23776)\\t1\\n  (0, 27074)\\t1</td>\n",
       "      <td>(0, 27074)\\t0.1239938922306029\\n  (0, 23776)\\t0.13513444070634492\\n  (0, 21643)\\t0.24470054908263625\\n  (0, 20980)\\t0.46126299652355346\\n  (0, 20536)\\t0.3804543283812734\\n  (0, 16492)\\t0.27026860460580737\\n  (0, 14305)\\t0.26512806634341224\\n  (0, 13034)\\t0.2918120507841267\\n  (0, 11040)\\t0.09238380284680382\\n  (0, 10804)\\t0.3537703439405589\\n  (0, 9678)\\t0.1906313431071623\\n  (0, 7025)\\t0.09748259501579978\\n  (0, 2689)\\t0.2045413073727332\\n  (0, 1310)\\t0.3148268933365545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                      comment  \\\n",
       "0                                                                                        Верблюдов-то за что? Дебилы, бл...\\n   \n",
       "1  Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было, кисель их бы придумал.\\n   \n",
       "\n",
       "                                                                                                clear  \\\n",
       "0                                                                                  [верблюд, деб, бл]   \n",
       "1  [хохол, эт, отдушин, затюкан, россиянин, мол, вон, хохл, ещ, плох, есл, хохл, быт, кисел, придума]   \n",
       "\n",
       "                                                                                                                                                                                                                                BoW_sklearn  \\\n",
       "0                                                                                                                                                                                            (0, 5172)\\t1\\n  (0, 16868)\\t1\\n  (0, 18339)\\t1   \n",
       "1    (0, 1310)\\t1\\n  (0, 2689)\\t1\\n  (0, 7025)\\t1\\n  (0, 9678)\\t1\\n  (0, 10804)\\t1\\n  (0, 11040)\\t1\\n  (0, 13034)\\t1\\n  (0, 14305)\\t1\\n  (0, 16492)\\t1\\n  (0, 20536)\\t1\\n  (0, 20980)\\t2\\n  (0, 21643)\\t1\\n  (0, 23776)\\t1\\n  (0, 27074)\\t1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           TFIDF  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                              (0, 18339)\\t0.4228355715889868\\n  (0, 16868)\\t0.6101326664930184\\n  (0, 5172)\\t0.6700359756588701  \n",
       "1    (0, 27074)\\t0.1239938922306029\\n  (0, 23776)\\t0.13513444070634492\\n  (0, 21643)\\t0.24470054908263625\\n  (0, 20980)\\t0.46126299652355346\\n  (0, 20536)\\t0.3804543283812734\\n  (0, 16492)\\t0.27026860460580737\\n  (0, 14305)\\t0.26512806634341224\\n  (0, 13034)\\t0.2918120507841267\\n  (0, 11040)\\t0.09238380284680382\\n  (0, 10804)\\t0.3537703439405589\\n  (0, 9678)\\t0.1906313431071623\\n  (0, 7025)\\t0.09748259501579978\\n  (0, 2689)\\t0.2045413073727332\\n  (0, 1310)\\t0.3148268933365545  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data[['comment','clear','BoW_sklearn', 'TFIDF']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 18339)\t0.4228355715889868\n",
      "  (0, 16868)\t0.6101326664930184\n",
      "  (0, 5172)\t0.6700359756588701\n"
     ]
    }
   ],
   "source": [
    "print(df_data.loc[0]['TFIDF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализовать tfidf датасета ./quora.txt, посчитать косинусное расстояние\n",
    "!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt -nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What TV shows or books help you read people's body language?\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(open(\"./quora.txt\", encoding=\"utf-8\"))\n",
    "data[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data=pd.DataFrame(data[0:100000], columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "steamer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def preproc(word):\n",
    "    res=' '.join([(steamer.stem(lemmatizer.lemmatize(w))) for w in tokenizer.tokenize(word)])\n",
    "    return res\n",
    "\n",
    "def custom_analyzer(p_str):\n",
    "    stop_words= stopwords.words('english')\n",
    "    word_marks = r'.+[а-яa-z]'\n",
    "    res=[]\n",
    "    v_list = [(steamer.stem(lemmatizer.lemmatize(w))) for w in tokenizer.tokenize(p_str)]\n",
    "    for w in v_list:\n",
    "        if (w not in stop_words) and (re.match(word_marks, w)):\n",
    "            res.append(re.sub(r'\\d+', '', w))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Count Vectorizing: 0:02:30.590789\n"
     ]
    }
   ],
   "source": [
    "start_time=datetime.now()\n",
    "cnt_vctrzr = CountVectorizer(lowercase=True\n",
    "                             #, stop_words='english'\n",
    "                             #, token_pattern=r'(?u)\\b[a-z]+\\b'\n",
    "                             #, preprocessor=preproc\n",
    "                             ,analyzer=custom_analyzer)\n",
    "words_cnt = cnt_vctrzr.fit_transform(data[0:100000])\n",
    "print('#Count Vectorizing:', datetime.now()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>WordsCnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can I get back with my ex even though she is pregnant with another guy's baby?\\n</td>\n",
       "      <td>(0, 9287)\\t1\\n  (0, 1825)\\t1\\n  (0, 7873)\\t1\\n  (0, 7832)\\t1\\n  (0, 23917)\\t1\\n  (0, 18607)\\t1\\n  (0, 1008)\\t1\\n  (0, 9917)\\t1\\n  (0, 1802)\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are some ways to overcome a fast food addiction?\\n</td>\n",
       "      <td>(0, 26065)\\t1\\n  (0, 17160)\\t1\\n  (0, 8157)\\t1\\n  (0, 8644)\\t1\\n  (0, 246)\\t1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               text  \\\n",
       "0  Can I get back with my ex even though she is pregnant with another guy's baby?\\n   \n",
       "1                           What are some ways to overcome a fast food addiction?\\n   \n",
       "\n",
       "                                                                                                                                           WordsCnt  \n",
       "0    (0, 9287)\\t1\\n  (0, 1825)\\t1\\n  (0, 7873)\\t1\\n  (0, 7832)\\t1\\n  (0, 23917)\\t1\\n  (0, 18607)\\t1\\n  (0, 1008)\\t1\\n  (0, 9917)\\t1\\n  (0, 1802)\\t1  \n",
       "1                                                                     (0, 26065)\\t1\\n  (0, 17160)\\t1\\n  (0, 8157)\\t1\\n  (0, 8644)\\t1\\n  (0, 246)\\t1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data['WordsCnt']=[words_cnt.getrow(i) for i in df_data.index]\n",
    "df_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#get tfidf: 0:00:30.848843\n"
     ]
    }
   ],
   "source": [
    "start_time=datetime.now()\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "words_tfidf=tfidf_transformer.fit(words_cnt)\n",
    "tf_idf_vector = tfidf_transformer.transform(words_cnt)\n",
    "tfidf_list = [sparse.coo_matrix(tf_idf_vector[i])  for i in df_data.index]\n",
    "df_data['TFIDF'] = tfidf_list\n",
    "\n",
    "print('#get tfidf:', datetime.now()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>whi</th>\n",
       "      <td>3.290271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>3.585091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doe</th>\n",
       "      <td>3.649037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>3.927336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>4.298200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      idf_weights\n",
       "whi      3.290271\n",
       "best     3.585091\n",
       "doe      3.649037\n",
       "get      3.927336\n",
       "like     4.298200"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cnt_vctrzr.get_feature_names_out(),columns=[\"idf_weights\"])\n",
    "df_idf.sort_values(by='idf_weights', ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>WordsCnt</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can I get back with my ex even though she is pregnant with another guy's baby?\\n</td>\n",
       "      <td>(0, 9287)\\t1\\n  (0, 1825)\\t1\\n  (0, 7873)\\t1\\n  (0, 7832)\\t1\\n  (0, 23917)\\t1\\n  (0, 18607)\\t1\\n  (0, 1008)\\t1\\n  (0, 9917)\\t1\\n  (0, 1802)\\t1</td>\n",
       "      <td>(0, 23917)\\t0.3820134662500762\\n  (0, 18607)\\t0.3801152898995122\\n  (0, 9917)\\t0.3049550171488711\\n  (0, 9287)\\t0.19603700588911063\\n  (0, 7873)\\t0.36164544499800044\\n  (0, 7832)\\t0.317795661488269\\n  (0, 1825)\\t0.30587246361423925\\n  (0, 1802)\\t0.3731713320605975\\n  (0, 1008)\\t0.3369772847063777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are some ways to overcome a fast food addiction?\\n</td>\n",
       "      <td>(0, 26065)\\t1\\n  (0, 17160)\\t1\\n  (0, 8157)\\t1\\n  (0, 8644)\\t1\\n  (0, 246)\\t1</td>\n",
       "      <td>(0, 26065)\\t0.30481710742151347\\n  (0, 17160)\\t0.49393044539767544\\n  (0, 8644)\\t0.42114970189628725\\n  (0, 8157)\\t0.474494516094805\\n  (0, 246)\\t0.5104969431064802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               text  \\\n",
       "0  Can I get back with my ex even though she is pregnant with another guy's baby?\\n   \n",
       "1                           What are some ways to overcome a fast food addiction?\\n   \n",
       "\n",
       "                                                                                                                                           WordsCnt  \\\n",
       "0    (0, 9287)\\t1\\n  (0, 1825)\\t1\\n  (0, 7873)\\t1\\n  (0, 7832)\\t1\\n  (0, 23917)\\t1\\n  (0, 18607)\\t1\\n  (0, 1008)\\t1\\n  (0, 9917)\\t1\\n  (0, 1802)\\t1   \n",
       "1                                                                     (0, 26065)\\t1\\n  (0, 17160)\\t1\\n  (0, 8157)\\t1\\n  (0, 8644)\\t1\\n  (0, 246)\\t1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                         TFIDF  \n",
       "0    (0, 23917)\\t0.3820134662500762\\n  (0, 18607)\\t0.3801152898995122\\n  (0, 9917)\\t0.3049550171488711\\n  (0, 9287)\\t0.19603700588911063\\n  (0, 7873)\\t0.36164544499800044\\n  (0, 7832)\\t0.317795661488269\\n  (0, 1825)\\t0.30587246361423925\\n  (0, 1802)\\t0.3731713320605975\\n  (0, 1008)\\t0.3369772847063777  \n",
       "1                                                                                                                                         (0, 26065)\\t0.30481710742151347\\n  (0, 17160)\\t0.49393044539767544\\n  (0, 8644)\\t0.42114970189628725\\n  (0, 8157)\\t0.474494516094805\\n  (0, 246)\\t0.5104969431064802  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26978\n"
     ]
    }
   ],
   "source": [
    "print(len(df_data.loc[0]['TFIDF'].toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#get cosine_similarity: 0:00:12.422452\n"
     ]
    }
   ],
   "source": [
    "#cosine_similarity\n",
    "srs_tfidf = df_data[0:5000]['TFIDF']\n",
    "tf_idf_vector2 = tf_idf_vector[0:5000]\n",
    "df_cos_dist = pd.DataFrame(index=df_data.index, columns=['max_cos_dist', 'max_cos_dist_index'])\n",
    "\n",
    "start_time=datetime.now()\n",
    "x=srs_tfidf[0]\n",
    "for i, xi in srs_tfidf.items():\n",
    "    cos_dist=cosine_similarity(xi, tf_idf_vector2)[0]\n",
    "    cos_dist[i]=0\n",
    "    #df_cos_dist.loc[i][['cos_dist', 'max_cos_dist', 'max_cos_dist_index']] = [cos_dist, np.max(cos_dist), np.argmax(cos_dist)] \n",
    "    df_cos_dist.loc[i][['max_cos_dist', 'max_cos_dist_index']] = [np.max(cos_dist), np.argmax(cos_dist)] \n",
    "    #if i%10==0:\n",
    "     #   print(i)\n",
    "        \n",
    "print('#get cosine_similarity:', datetime.now()-start_time)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the most similar strings:\n",
      "4397                How can I become intelligent?\\n\n",
      "3783    What can I do to become more intelligent?\\n\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "similar_idx = df_cos_dist.sort_values(by='max_cos_dist', ascending=False).head(1)['max_cos_dist_index']\n",
    "x=similar_idx.index[0]\n",
    "y=similar_idx.values[0]\n",
    "print('the most similar strings:')\n",
    "print(df_data.loc[[x,y]]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
